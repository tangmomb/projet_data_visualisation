{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a655be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427aee",
   "metadata": {},
   "source": [
    "### Infos du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d04fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d930ff",
   "metadata": {},
   "source": [
    "### Doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33905001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for strictly identical rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'Number of strictly identical rows: {num_duplicates}')\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print('Duplicate rows:')\n",
    "    print(df[df.duplicated()])\n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d3f67",
   "metadata": {},
   "source": [
    "### Types de variables avant conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1b35",
   "metadata": {},
   "source": [
    "### Conversion des types des variables et création du parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "# tout convertir\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# puis les dates\n",
    "date_columns = ['time', 'updated']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "df.to_parquet('data/STEP01_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf768b",
   "metadata": {},
   "source": [
    "### Vérification conversion des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc010dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dc092",
   "metadata": {},
   "source": [
    "### Les valeurs 0 de nst et magNst passent en valeurs vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "# Remplacer les 0 par NaN dans nst et magNst\n",
    "df['nst'] = df['nst'].replace(0, pd.NA)\n",
    "df['magNst'] = df['magNst'].replace(0, pd.NA)\n",
    "\n",
    "# Sauvegarder le parquet mis à jour\n",
    "df.to_parquet('data/STEP02_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815d8ab",
   "metadata": {},
   "source": [
    "### Vérification valeurs faussement différentes dans \"place\" (ex : Washington, washington, Washington DC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17026301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n",
    "\n",
    "# Compter les occurrences de chaque place\n",
    "place_counts = df['place'].value_counts()\n",
    "\n",
    "# Places qui apparaissent plus d'une fois\n",
    "duplicates = place_counts[place_counts > 1]\n",
    "\n",
    "print(f\"Nombre de places qui apparaissent plus d'une fois : {len(duplicates)}\")\n",
    "print(f\"Total d'occurrences pour ces places : {duplicates.sum()}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Exemples de places avec leurs occurrences :\")\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_place(place):\n",
    "    if pd.isna(place):\n",
    "        return place\n",
    "    # Remove accents\n",
    "    place = unicodedata.normalize('NFD', place).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    place = place.lower()\n",
    "    # Remove punctuation\n",
    "    place = re.sub(r'[^\\w\\s]', '', place)\n",
    "    # Remove extra spaces\n",
    "    place = ' '.join(place.split())\n",
    "    return place\n",
    "\n",
    "df = pd.read_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "df['original_place'] = df['place']\n",
    "\n",
    "# Apply cleaning to the 'place' column\n",
    "df['place'] = df['place'].apply(clean_place)\n",
    "\n",
    "# Count unique cleaned places\n",
    "unique_clean = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' après nettoyage : {unique_clean}\")\n",
    "\n",
    "# Group by cleaned place and show examples of groupings\n",
    "grouped = df.groupby('place')['original_place'].unique()\n",
    "grouped_multiple = grouped[grouped.apply(len) > 1]\n",
    "\n",
    "print(\"Exemples de lieux regroupés après nettoyage :\")\n",
    "for cleaned, originals in grouped_multiple.head(10).items():\n",
    "    print(f\"Nettoyé: '{cleaned}'\")\n",
    "    print(f\"Originaux: {list(originals)}\")\n",
    "    print(\"---\")\n",
    "\n",
    "# Enregistrer le parquet avec la colonne 'place' nettoyée\n",
    "df.to_parquet('data/STEP03_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06820b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce9333",
   "metadata": {},
   "source": [
    "### Ajout colonne magnitude uniformisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d0e73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveau fichier 'data/STEP04_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_magnitude(row):\n",
    "    mag = row[\"mag\"]\n",
    "    mtype = row[\"magType\"]\n",
    "\n",
    "    if pd.isna(mag) or pd.isna(mtype):\n",
    "        return np.nan\n",
    "\n",
    "    mtype = str(mtype).lower()  # normalisation\n",
    "\n",
    "    if mtype in [\"ml\", \"mlg\", \"mlr\"]:  \n",
    "        return mag\n",
    "\n",
    "    if mtype in [\"md\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"mw\", \"mwc\", \"mwb\", \"mwr\", \"mww\"]:\n",
    "        return (mag - 0.9) / 0.67\n",
    "\n",
    "    if mtype in [\"mc\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"ma\"]:\n",
    "        return mag  \n",
    "\n",
    "    return pd.NA\n",
    "\n",
    "\n",
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Appliquer la fonction de conversion\n",
    "df['mag_uniform'] = df.apply(convert_magnitude, axis=1)\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "print(\"Nouveau fichier 'data/STEP04_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c1b3",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2923bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes par colonne :\n",
      "time                     0\n",
      "latitude                 0\n",
      "longitude                0\n",
      "depth                    9\n",
      "mag                 156449\n",
      "magType             167407\n",
      "nst                1211162\n",
      "gap                 838549\n",
      "dmin               1346742\n",
      "rms                 211653\n",
      "net                      0\n",
      "id                       0\n",
      "updated                  0\n",
      "place                   11\n",
      "type                     0\n",
      "horizontalError    1531963\n",
      "depthError          606685\n",
      "magError           1781012\n",
      "magNst             1096838\n",
      "status                   1\n",
      "locationSource           0\n",
      "magSource                0\n",
      "original_place          11\n",
      "mag_uniform         704865\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc443b1",
   "metadata": {},
   "source": [
    "### Création d'un petit parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32435778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Extraire les 10 premières lignes\n",
    "df_lite = df.head(10)\n",
    "\n",
    "# Sauvegarder en parquet lite\n",
    "df_lite.to_parquet('data/earthquakes_lite.parquet')\n",
    "\n",
    "print(\"Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
