{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a655be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427aee",
   "metadata": {},
   "source": [
    "### Infos du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d04fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('csv/earthquakes.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d930ff",
   "metadata": {},
   "source": [
    "### Doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33905001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for strictly identical rows\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'Number of strictly identical rows: {num_duplicates}')\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print('Duplicate rows:')\n",
    "    print(df[df.duplicated()])\n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d3f67",
   "metadata": {},
   "source": [
    "### Types de variables avant conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/earthquakes.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1b35",
   "metadata": {},
   "source": [
    "### Conversion des variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/earthquakes.csv')\n",
    "\n",
    "# tout convertir\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# puis les dates\n",
    "date_columns = ['time', 'updated']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "df.to_parquet('csv/earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf768b",
   "metadata": {},
   "source": [
    "### Vérification conversion des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/earthquakes.csv')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc010dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c1b3",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2923bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes par colonne :\n",
      "time                     0\n",
      "latitude                 0\n",
      "longitude                0\n",
      "depth                    9\n",
      "mag                 156449\n",
      "magType             167407\n",
      "nst                 881566\n",
      "gap                 838549\n",
      "dmin               1346742\n",
      "rms                 211653\n",
      "net                      0\n",
      "id                       0\n",
      "updated                  0\n",
      "place                   11\n",
      "type                     0\n",
      "horizontalError    1531963\n",
      "depthError          606685\n",
      "magError           1781012\n",
      "magNst              988917\n",
      "status                   1\n",
      "locationSource           0\n",
      "magSource                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81abb0",
   "metadata": {},
   "source": [
    "### Nombre de valeurs uniques (places vs coordonnées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c82e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs uniques dans 'latitude' : 419761\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans la colonne 'latitude'\n",
    "unique_latitudes = df['latitude'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'latitude' : {unique_latitudes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ced06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs uniques dans 'place' : 205436\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans la colonne 'latitude'\n",
    "unique_places = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' : {unique_places}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc443b1",
   "metadata": {},
   "source": [
    "### Copie légère du parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32435778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'csv/earthquakes_lite.parquet' créé avec les 10 premières lignes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('csv/earthquakes.parquet')\n",
    "\n",
    "# Extraire les 10 premières lignes\n",
    "df_lite = df.head(10)\n",
    "\n",
    "# Sauvegarder en parquet lite\n",
    "df_lite.to_parquet('csv/earthquakes_lite.parquet')\n",
    "\n",
    "print(\"Fichier 'csv/earthquakes_lite.parquet' créé avec les 10 premières lignes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
