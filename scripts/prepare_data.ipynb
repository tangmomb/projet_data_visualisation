{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a655be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427aee",
   "metadata": {},
   "source": [
    "### Infos du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4d04fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3272774, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>net</th>\n",
       "      <th>id</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>37.003502</td>\n",
       "      <td>-117.996834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci37038459</td>\n",
       "      <td>2016-04-02T20:22:05.312Z</td>\n",
       "      <td>29km NE of Independence, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>35.642788</td>\n",
       "      <td>-120.933601</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>mh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci11092098</td>\n",
       "      <td>2016-01-29T01:43:14.870Z</td>\n",
       "      <td>11km SSW of Lake Nacimiento, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>34.164520</td>\n",
       "      <td>-118.185036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci15086796</td>\n",
       "      <td>2016-04-02T17:20:31.235Z</td>\n",
       "      <td>4km S of La Canada Flintridge, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>33.836494</td>\n",
       "      <td>-116.781868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci14891508</td>\n",
       "      <td>2016-04-02T14:10:48.389Z</td>\n",
       "      <td>9km S of Cabazon, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>33.208477</td>\n",
       "      <td>-115.476997</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci10925125</td>\n",
       "      <td>2016-04-02T04:32:22.103Z</td>\n",
       "      <td>5km SE of Niland, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time   latitude   longitude  depth   mag magType  nst  \\\n",
       "0  1970-01-01T00:00:00.0Z  37.003502 -117.996834    0.0  0.00      mh  0.0   \n",
       "1  1970-01-01T00:00:00.0Z  35.642788 -120.933601    5.0  1.99      mh  2.0   \n",
       "2  1970-01-01T00:00:00.0Z  34.164520 -118.185036    0.0  0.00      mh  NaN   \n",
       "3  1970-01-01T00:00:00.0Z  33.836494 -116.781868    0.0  0.00      mh  NaN   \n",
       "4  1970-01-01T00:00:00.0Z  33.208477 -115.476997    5.0  0.00      mh  NaN   \n",
       "\n",
       "   gap  dmin  rms net          id                   updated  \\\n",
       "0  NaN   NaN  NaN  ci  ci37038459  2016-04-02T20:22:05.312Z   \n",
       "1  NaN   NaN  NaN  ci  ci11092098  2016-01-29T01:43:14.870Z   \n",
       "2  NaN   NaN  NaN  ci  ci15086796  2016-04-02T17:20:31.235Z   \n",
       "3  NaN   NaN  NaN  ci  ci14891508  2016-04-02T14:10:48.389Z   \n",
       "4  NaN   NaN  NaN  ci  ci10925125  2016-04-02T04:32:22.103Z   \n",
       "\n",
       "                               place        type  horizontalError  depthError  \\\n",
       "0        29km NE of Independence, CA  sonic boom              NaN         NaN   \n",
       "1    11km SSW of Lake Nacimiento, CA  earthquake              NaN         NaN   \n",
       "2  4km S of La Canada Flintridge, CA  earthquake              NaN         NaN   \n",
       "3               9km S of Cabazon, CA  sonic boom              NaN         NaN   \n",
       "4               5km SE of Niland, CA  sonic boom              NaN         NaN   \n",
       "\n",
       "   magError  magNst    status locationSource magSource  \n",
       "0       NaN     0.0  reviewed             ci        ci  \n",
       "1       NaN     0.0  reviewed             ci        ci  \n",
       "2       NaN     0.0  reviewed             ci        ci  \n",
       "3       NaN     0.0  reviewed             ci        ci  \n",
       "4       NaN     0.0  reviewed             ci        ci  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../data/earthquakes.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d3f67",
   "metadata": {},
   "source": [
    "### Types de variables avant conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68f5b3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                object\n",
       "latitude           float64\n",
       "longitude          float64\n",
       "depth              float64\n",
       "mag                float64\n",
       "magType             object\n",
       "nst                float64\n",
       "gap                float64\n",
       "dmin               float64\n",
       "rms                float64\n",
       "net                 object\n",
       "id                  object\n",
       "updated             object\n",
       "place               object\n",
       "type                object\n",
       "horizontalError    float64\n",
       "depthError         float64\n",
       "magError           float64\n",
       "magNst             float64\n",
       "status              object\n",
       "locationSource      object\n",
       "magSource           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/earthquakes.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1b35",
   "metadata": {},
   "source": [
    "### Conversion des types des variables et création du parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e95181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/earthquakes.csv')\n",
    "\n",
    "# tout convertir\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# puis les dates\n",
    "date_columns = ['time', 'updated']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "df.to_parquet('../data/STEP01_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d90b1775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time               datetime64[ns, UTC]\n",
       "latitude                       Float64\n",
       "longitude                      Float64\n",
       "depth                          Float64\n",
       "mag                            Float64\n",
       "magType                 string[python]\n",
       "nst                              Int64\n",
       "gap                            Float64\n",
       "dmin                           Float64\n",
       "rms                            Float64\n",
       "net                     string[python]\n",
       "id                      string[python]\n",
       "updated            datetime64[ns, UTC]\n",
       "place                   string[python]\n",
       "type                    string[python]\n",
       "horizontalError                Float64\n",
       "depthError                     Float64\n",
       "magError                       Float64\n",
       "magNst                           Int64\n",
       "status                  string[python]\n",
       "locationSource          string[python]\n",
       "magSource               string[python]\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP01_earthquakes.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf768b",
   "metadata": {},
   "source": [
    "### Vérification conversion des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94131a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs vides dans 'time': 0\n",
      "Nombre de valeurs vides dans 'updated': 0\n",
      "Nombre total de lignes dans le fichier: 3272774\n",
      "Nombre de variables de type date dans 'time': 3272774\n",
      "Nombre de variables de type date dans 'updated': 3272774\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP01_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")\n",
    "\n",
    "# Compter le nombre de lignes du fichier\n",
    "total_rows = len(df)\n",
    "print(f\"Nombre total de lignes dans le fichier: {total_rows}\")\n",
    "\n",
    "# Compter le nombre de variables de type date (non-null) dans 'time' et 'updated'\n",
    "time_date_count = df['time'].notnull().sum()\n",
    "updated_date_count = df['updated'].notnull().sum()\n",
    "\n",
    "print(f\"Nombre de variables de type date dans 'time': {time_date_count}\")\n",
    "print(f\"Nombre de variables de type date dans 'updated': {updated_date_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cc592",
   "metadata": {},
   "source": [
    "### Doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8b184a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strictly identical rows: 15819\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>net</th>\n",
       "      <th>id</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1970-03-01 00:49:06.230000+00:00</td>\n",
       "      <td>33.122</td>\n",
       "      <td>-117.6545</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>ml</td>\n",
       "      <td>9</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.7027</td>\n",
       "      <td>0.68</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324642</td>\n",
       "      <td>2016-01-29 01:38:30.170000+00:00</td>\n",
       "      <td>28km WSW of Camp Pendleton South, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2.99</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.319</td>\n",
       "      <td>5</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1970-03-01 00:49:06.230000+00:00</td>\n",
       "      <td>33.122</td>\n",
       "      <td>-117.6545</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>ml</td>\n",
       "      <td>9</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.7027</td>\n",
       "      <td>0.68</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324642</td>\n",
       "      <td>2016-01-29 01:38:30.170000+00:00</td>\n",
       "      <td>28km WSW of Camp Pendleton South, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>2.99</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.319</td>\n",
       "      <td>5</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1970-03-01 04:14:14.310000+00:00</td>\n",
       "      <td>35.4065</td>\n",
       "      <td>-117.9545</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.54</td>\n",
       "      <td>mh</td>\n",
       "      <td>4</td>\n",
       "      <td>258.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.38</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324643</td>\n",
       "      <td>2016-01-29 01:33:57.060000+00:00</td>\n",
       "      <td>29km W of Johannesburg, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>3.18</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.205</td>\n",
       "      <td>13</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1970-03-01 04:14:14.310000+00:00</td>\n",
       "      <td>35.4065</td>\n",
       "      <td>-117.9545</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.54</td>\n",
       "      <td>mh</td>\n",
       "      <td>4</td>\n",
       "      <td>258.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.38</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324643</td>\n",
       "      <td>2016-01-29 01:33:57.060000+00:00</td>\n",
       "      <td>29km W of Johannesburg, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>3.18</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.205</td>\n",
       "      <td>13</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1970-03-01 05:44:23.110000+00:00</td>\n",
       "      <td>33.987833</td>\n",
       "      <td>-118.430833</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.44</td>\n",
       "      <td>ml</td>\n",
       "      <td>5</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.49</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324644</td>\n",
       "      <td>2016-01-29 01:08:57.240000+00:00</td>\n",
       "      <td>2km ENE of Marina del Rey, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>3.51</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.034</td>\n",
       "      <td>3</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1970-03-01 05:44:23.110000+00:00</td>\n",
       "      <td>33.987833</td>\n",
       "      <td>-118.430833</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.44</td>\n",
       "      <td>ml</td>\n",
       "      <td>5</td>\n",
       "      <td>248.0</td>\n",
       "      <td>0.2686</td>\n",
       "      <td>0.49</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci3324644</td>\n",
       "      <td>2016-01-29 01:08:57.240000+00:00</td>\n",
       "      <td>2km ENE of Marina del Rey, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>3.51</td>\n",
       "      <td>31.61</td>\n",
       "      <td>0.034</td>\n",
       "      <td>3</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1970-03-01 10:51:11.580000+00:00</td>\n",
       "      <td>46.881</td>\n",
       "      <td>-119.424167</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>2.3</td>\n",
       "      <td>md</td>\n",
       "      <td>7</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.06</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw10836218</td>\n",
       "      <td>2016-07-24 23:40:23.120000+00:00</td>\n",
       "      <td>Washington</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1970-03-01 10:51:11.580000+00:00</td>\n",
       "      <td>46.881</td>\n",
       "      <td>-119.424167</td>\n",
       "      <td>-0.261</td>\n",
       "      <td>2.3</td>\n",
       "      <td>md</td>\n",
       "      <td>7</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.06</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw10836218</td>\n",
       "      <td>2016-07-24 23:40:23.120000+00:00</td>\n",
       "      <td>Washington</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1970-03-02 12:33:40.480000+00:00</td>\n",
       "      <td>46.8765</td>\n",
       "      <td>-119.4225</td>\n",
       "      <td>2.729</td>\n",
       "      <td>1.4</td>\n",
       "      <td>md</td>\n",
       "      <td>4</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.04</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw10836223</td>\n",
       "      <td>2016-07-24 23:40:23.350000+00:00</td>\n",
       "      <td>Washington</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1970-03-02 12:33:40.480000+00:00</td>\n",
       "      <td>46.8765</td>\n",
       "      <td>-119.4225</td>\n",
       "      <td>2.729</td>\n",
       "      <td>1.4</td>\n",
       "      <td>md</td>\n",
       "      <td>4</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.04</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw10836223</td>\n",
       "      <td>2016-07-24 23:40:23.350000+00:00</td>\n",
       "      <td>Washington</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.377</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>uw</td>\n",
       "      <td>uw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                time   latitude   longitude  depth   mag  \\\n",
       "187 1970-03-01 00:49:06.230000+00:00     33.122   -117.6545    6.0  2.98   \n",
       "188 1970-03-01 00:49:06.230000+00:00     33.122   -117.6545    6.0  2.98   \n",
       "189 1970-03-01 04:14:14.310000+00:00    35.4065   -117.9545    6.0  2.54   \n",
       "190 1970-03-01 04:14:14.310000+00:00    35.4065   -117.9545    6.0  2.54   \n",
       "191 1970-03-01 05:44:23.110000+00:00  33.987833 -118.430833    6.0  3.44   \n",
       "192 1970-03-01 05:44:23.110000+00:00  33.987833 -118.430833    6.0  3.44   \n",
       "193 1970-03-01 10:51:11.580000+00:00     46.881 -119.424167 -0.261   2.3   \n",
       "194 1970-03-01 10:51:11.580000+00:00     46.881 -119.424167 -0.261   2.3   \n",
       "195 1970-03-02 12:33:40.480000+00:00    46.8765   -119.4225  2.729   1.4   \n",
       "196 1970-03-02 12:33:40.480000+00:00    46.8765   -119.4225  2.729   1.4   \n",
       "\n",
       "    magType  nst    gap    dmin   rms net          id  \\\n",
       "187      ml    9  217.0  0.7027  0.68  ci   ci3324642   \n",
       "188      ml    9  217.0  0.7027  0.68  ci   ci3324642   \n",
       "189      mh    4  258.0    <NA>  0.38  ci   ci3324643   \n",
       "190      mh    4  258.0    <NA>  0.38  ci   ci3324643   \n",
       "191      ml    5  248.0  0.2686  0.49  ci   ci3324644   \n",
       "192      ml    5  248.0  0.2686  0.49  ci   ci3324644   \n",
       "193      md    7  145.0  0.1349  0.06  uw  uw10836218   \n",
       "194      md    7  145.0  0.1349  0.06  uw  uw10836218   \n",
       "195      md    4  229.0  0.1355  0.04  uw  uw10836223   \n",
       "196      md    4  229.0  0.1355  0.04  uw  uw10836223   \n",
       "\n",
       "                             updated                                 place  \\\n",
       "187 2016-01-29 01:38:30.170000+00:00  28km WSW of Camp Pendleton South, CA   \n",
       "188 2016-01-29 01:38:30.170000+00:00  28km WSW of Camp Pendleton South, CA   \n",
       "189 2016-01-29 01:33:57.060000+00:00            29km W of Johannesburg, CA   \n",
       "190 2016-01-29 01:33:57.060000+00:00            29km W of Johannesburg, CA   \n",
       "191 2016-01-29 01:08:57.240000+00:00         2km ENE of Marina del Rey, CA   \n",
       "192 2016-01-29 01:08:57.240000+00:00         2km ENE of Marina del Rey, CA   \n",
       "193 2016-07-24 23:40:23.120000+00:00                            Washington   \n",
       "194 2016-07-24 23:40:23.120000+00:00                            Washington   \n",
       "195 2016-07-24 23:40:23.350000+00:00                            Washington   \n",
       "196 2016-07-24 23:40:23.350000+00:00                            Washington   \n",
       "\n",
       "           type  horizontalError  depthError  magError  magNst    status  \\\n",
       "187  earthquake             2.99       31.61     0.319       5  reviewed   \n",
       "188  earthquake             2.99       31.61     0.319       5  reviewed   \n",
       "189  earthquake             3.18       31.61     0.205      13  reviewed   \n",
       "190  earthquake             3.18       31.61     0.205      13  reviewed   \n",
       "191  earthquake             3.51       31.61     0.034       3  reviewed   \n",
       "192  earthquake             3.51       31.61     0.034       3  reviewed   \n",
       "193  earthquake            0.022        0.04      0.14       0  reviewed   \n",
       "194  earthquake            0.022        0.04      0.14       0  reviewed   \n",
       "195  earthquake            0.377        1.45      0.13       0  reviewed   \n",
       "196  earthquake            0.377        1.45      0.13       0  reviewed   \n",
       "\n",
       "    locationSource magSource  \n",
       "187             ci        ci  \n",
       "188             ci        ci  \n",
       "189             ci        ci  \n",
       "190             ci        ci  \n",
       "191             ci        ci  \n",
       "192             ci        ci  \n",
       "193             uw        uw  \n",
       "194             uw        uw  \n",
       "195             uw        uw  \n",
       "196             uw        uw  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for strictly identical rows\n",
    "df = pd.read_parquet('../data/STEP01_earthquakes.parquet')\n",
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'Number of strictly identical rows: {num_duplicates}')\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    duplicated_rows = df[df.duplicated(keep=False)]\n",
    "    display(duplicated_rows.head(10)) \n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b976b5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 'id' entries: 3272774\n",
      "Unique 'id' entries: 3256955\n",
      "Number of identical 'id's (duplicates): 15819\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate 'id' values\n",
    "df = pd.read_parquet('../data/STEP01_earthquakes.parquet')\n",
    "total_ids = len(df)\n",
    "unique_ids = df['id'].nunique()\n",
    "duplicate_ids_count = total_ids - unique_ids\n",
    "\n",
    "print(f\"Total 'id' entries: {total_ids}\")\n",
    "print(f\"Unique 'id' entries: {unique_ids}\")\n",
    "print(f\"Number of identical 'id's (duplicates): {duplicate_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "000aec56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier '../data/STEP02_earthquakes.parquet' créé sans doublons.\n",
      "Nombre de doublons supprimés : 15819\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP01_earthquakes.parquet')\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Supprimer les doublons\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "after = len(df)\n",
    "\n",
    "# Calculer le nombre de doublons supprimés\n",
    "removed = before - after\n",
    "\n",
    "df.to_parquet('../data/STEP02_earthquakes.parquet')\n",
    "\n",
    "print(f\"Fichier '../data/STEP02_earthquakes.parquet' créé sans doublons.\")\n",
    "print(f\"Nombre de doublons supprimés : {removed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dc092",
   "metadata": {},
   "source": [
    "### Les valeurs 0 de nst et magNst passent en valeurs vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c31a4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/STEP02_earthquakes.parquet')\n",
    "\n",
    "# Remplacer les 0 par NaN dans nst et magNst\n",
    "df['nst'] = df['nst'].replace(0, pd.NA)\n",
    "df['magNst'] = df['magNst'].replace(0, pd.NA)\n",
    "\n",
    "# Sauvegarder le parquet mis à jour\n",
    "df.to_parquet('../data/STEP03_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815d8ab",
   "metadata": {},
   "source": [
    "### Vérification valeurs faussement différentes dans \"place\" (ex : central East Pacific Rise, Central East Pacific Rise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17026301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs uniques dans 'place' original : 205436\n",
      "Nombre de places qui apparaissent plus d'une fois : 87828\n",
      "Total d'occurrences pour ces places : 3139336\n",
      "Exemples de places avec leurs occurrences :\n",
      "place\n",
      "Northern California                   354786\n",
      "Central California                    276036\n",
      "Central Alaska                        119187\n",
      "Nevada                                101748\n",
      "Long Valley area, California           87858\n",
      "Southern Alaska                        72772\n",
      "Utah                                   42260\n",
      "Washington                             39870\n",
      "San Francisco Bay area, California     28438\n",
      "Mount St. Helens area, Washington      28082\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n",
    "\n",
    "# Compter les occurrences de chaque place\n",
    "place_counts = df['place'].value_counts()\n",
    "\n",
    "# Places qui apparaissent plus d'une fois\n",
    "duplicates = place_counts[place_counts > 1]\n",
    "\n",
    "print(f\"Nombre de places qui apparaissent plus d'une fois : {len(duplicates)}\")\n",
    "print(f\"Total d'occurrences pour ces places : {duplicates.sum()}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Exemples de places avec leurs occurrences :\")\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "328b1809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs uniques dans 'place' après nettoyage : 205355\n",
      "Exemples de lieux regroupés après nettoyage :\n",
      "Nettoyé: 'central east pacific rise'\n",
      "Originaux: ['central East Pacific Rise', 'Central East Pacific Rise']\n",
      "---\n",
      "Nettoyé: 'central midatlantic ridge'\n",
      "Originaux: ['central Mid-Atlantic Ridge', 'Central Mid-Atlantic Ridge']\n",
      "---\n",
      "Nettoyé: 'east central pacific ocean'\n",
      "Originaux: ['east central Pacific Ocean', 'East central Pacific Ocean']\n",
      "---\n",
      "Nettoyé: 'east of severnaya zemlya'\n",
      "Originaux: ['east of Severnaya Zemlya', 'East of Severnaya Zemlya']\n",
      "---\n",
      "Nettoyé: 'east of the kuril islands'\n",
      "Originaux: ['east of the Kuril Islands', 'East of the Kuril Islands']\n",
      "---\n",
      "Nettoyé: 'east of the mariana islands'\n",
      "Originaux: ['east of the Mariana Islands', 'East of the Mariana Islands']\n",
      "---\n",
      "Nettoyé: 'east of the north island of new zealand'\n",
      "Originaux: ['east of the North Island of New Zealand', 'East of the North Island of New Zealand']\n",
      "---\n",
      "Nettoyé: 'east of the philippine islands'\n",
      "Originaux: ['east of the Philippine Islands', 'East of the Philippine Islands']\n",
      "---\n",
      "Nettoyé: 'east of the south sandwich islands'\n",
      "Originaux: ['east of the South Sandwich Islands', 'East of the South Sandwich Islands']\n",
      "---\n",
      "Nettoyé: 'east of the volcano islands'\n",
      "Originaux: ['east of the Volcano Islands', 'East of the Volcano Islands']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_place(place):\n",
    "    if pd.isna(place):\n",
    "        return place\n",
    "    # Remove accents\n",
    "    place = unicodedata.normalize('NFD', place).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    place = place.lower()\n",
    "    # Remove punctuation\n",
    "    place = re.sub(r'[^\\w\\s]', '', place)\n",
    "    # Remove extra spaces\n",
    "    place = ' '.join(place.split())\n",
    "    return place\n",
    "\n",
    "df = pd.read_parquet('../data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Apply cleaning to the 'place' column\n",
    "df['place_cleaned'] = df['place'].apply(clean_place)\n",
    "\n",
    "# Count unique cleaned places\n",
    "unique_clean = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' après nettoyage : {unique_clean}\")\n",
    "\n",
    "# Group by cleaned place and show examples of groupings\n",
    "grouped = df.groupby('place_cleaned')['place'].unique()\n",
    "grouped_multiple = grouped[grouped.apply(len) > 1]\n",
    "\n",
    "print(\"Exemples de lieux regroupés après nettoyage :\")\n",
    "for cleaned, originals in grouped_multiple.head(10).items():\n",
    "    print(f\"Nettoyé: '{cleaned}'\")\n",
    "    print(f\"Originaux: {list(originals)}\")\n",
    "    print(\"---\")\n",
    "\n",
    "df = df.drop(columns=['place'])\n",
    "\n",
    "# Enregistrer le parquet avec la colonne 'place' nettoyée\n",
    "df.to_parquet('../data/STEP04_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06820b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs uniques dans 'place' original : 205355\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce9333",
   "metadata": {},
   "source": [
    "### Ajout colonne magnitude uniformisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d0e73ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveau fichier '../data/STEP05_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_magnitude(row):\n",
    "    mag = row[\"mag\"]\n",
    "    mtype = row[\"magType\"]\n",
    "\n",
    "    if pd.isna(mag) or pd.isna(mtype):\n",
    "        return np.nan\n",
    "\n",
    "    mtype = str(mtype).lower()  # normalisation\n",
    "\n",
    "    if mtype in [\"ml\", \"mlg\", \"mlr\"]:  \n",
    "        return mag\n",
    "\n",
    "    if mtype in [\"md\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"mw\", \"mwc\", \"mwb\", \"mwr\", \"mww\"]:\n",
    "        return (mag - 0.9) / 0.67\n",
    "\n",
    "    if mtype in [\"mc\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"ma\"]:\n",
    "        return mag  \n",
    "\n",
    "    return pd.NA\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet('../data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Appliquer la fonction de conversion\n",
    "df['mag_uniform'] = df.apply(convert_magnitude, axis=1)\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('../data/STEP05_earthquakes.parquet')\n",
    "\n",
    "print(\"Nouveau fichier '../data/STEP05_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a596417",
   "metadata": {},
   "source": [
    "### Suppression colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3654de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/STEP05_earthquakes.parquet')\n",
    "\n",
    "df = df.drop(columns=['net', 'locationSource', 'magSource', 'status', 'dmin'])\n",
    "\n",
    "df.to_parquet('../data/STEP06_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa295",
   "metadata": {},
   "source": [
    "### Réorganisation du parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c8b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger ton fichier\n",
    "df = pd.read_parquet(\"../data/STEP06_earthquakes.parquet\")\n",
    "\n",
    "# Dictionnaire de renommage\n",
    "rename_map = {\n",
    "    \"id\": \"ID\",\n",
    "    \"time\": \"date\",\n",
    "    \"updated\": \"date_maj_infos\",\n",
    "\n",
    "    \"depth\": \"profondeur_km\",\n",
    "\n",
    "    \"mag\": \"magnitude\",\n",
    "    \"magType\": \"type_magnitude\",\n",
    "    \"magError\": \"erreur_magnitude\",\n",
    "    \"magNst\": \"nb_stations_magnitude\",\n",
    "\n",
    "    \"nst\": \"nb_stations_localisation\",\n",
    "    \"gap\": \"ecart_azimut\",\n",
    "    \"rms\": \"rms\",\n",
    "\n",
    "    \"horizontalError\": \"erreur_horiz\",\n",
    "    \"depthError\": \"erreur_profondeur\",\n",
    "\n",
    "    \"place_cleaned\": \"lieu\",\n",
    "    \"mag_uniform\": \"mag_uniforme\"\n",
    "}\n",
    "\n",
    "# Renommer les colonnes\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Nouvel ordre des colonnes\n",
    "new_order = [\n",
    "    \"ID\", \"date\",\"lieu\",\"magnitude\", \"type_magnitude\",\"latitude\", \"longitude\", \"profondeur_km\",  \"mag_uniforme\",\"nb_stations_localisation\", \"nb_stations_magnitude\",\"ecart_azimut\", \"rms\",\"erreur_horiz\",\"erreur_profondeur\",\"erreur_magnitude\",\"reseau\",\"type\", \"date_maj_infos\"\n",
    "]\n",
    "\n",
    "# Garder uniquement les colonnes existantes\n",
    "new_order = [col for col in new_order if col in df.columns]\n",
    "\n",
    "# Réorganiser\n",
    "df = df[new_order]\n",
    "\n",
    "df.to_parquet('../data/STEP07_earthquakes.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb4c48",
   "metadata": {},
   "source": [
    "### Suppression des évènements autres que \"earthquake\" et suppression de la colonne \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c55f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier '../data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP07_earthquakes.parquet')\n",
    "\n",
    "# Filtrer pour garder uniquement les événements de type 'earthquake'\n",
    "df = df[df['type'] == 'earthquake']\n",
    "\n",
    "# Supprimer la colonne 'type'\n",
    "df = df.drop(columns=['type'])\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('../data/STEP08_earthquakes.parquet')\n",
    "\n",
    "print(\"Fichier '../data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af5112",
   "metadata": {},
   "source": [
    "### Garder de 2000 à 2005 uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "577f0453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes entre 2000 et 2005 : 541525\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les lignes dont la date est comprise entre 2000 et 2005 inclus\n",
    "df = pd.read_parquet('../data/STEP08_earthquakes.parquet')\n",
    "\n",
    "mask_2000_2005 = (df['date'] >= '2000-01-01') & (df['date'] < '2006-01-01')\n",
    "df_2000_2005 = df[mask_2000_2005]\n",
    "\n",
    "print(f\"Nombre de lignes entre 2000 et 2005 : {len(df_2000_2005)}\")\n",
    "df_2000_2005.head()\n",
    "\n",
    "df_2000_2005.to_parquet('../data/STEP09_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43fa7d",
   "metadata": {},
   "source": [
    "### Garder uniquement les USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "774a6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 380123 lignes exportées\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "df = pd.read_parquet(\"../data/STEP09_earthquakes.parquet\")\n",
    "\n",
    "# Convertir en GeoDataFrame (WGS84)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Charger les pays\n",
    "world = gpd.read_file(\n",
    "    \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    ")\n",
    "\n",
    "usa = world[world[\"ADMIN\"] == \"United States of America\"]\n",
    "\n",
    "# Projection métrique US\n",
    "gdf_m = gdf.to_crs(\"EPSG:5070\")   # mètres\n",
    "usa_m = usa.to_crs(\"EPSG:5070\")\n",
    "\n",
    "# Buffer de 50 km autour des USA\n",
    "usa_buffer = usa_m.geometry.buffer(50 * 1000)\n",
    "\n",
    "# Filtrage spatial\n",
    "mask = gdf_m.geometry.within(usa_buffer.iloc[0])\n",
    "filtered = gdf.loc[mask].copy()\n",
    "\n",
    "# Export parquet\n",
    "filtered.drop(columns=\"geometry\").to_parquet(\"../data/STEP10_earthquakes.parquet\")\n",
    "\n",
    "print(f\"✅ {len(filtered)} lignes exportées\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04651",
   "metadata": {},
   "source": [
    "### Ressenti ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ca13594",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'ressenti' créée et fichier '../data/STEP11_earthquakes.parquet' sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP10_earthquakes.parquet')\n",
    "\n",
    "# Création de la colonne \"ressenti\" selon les critères donnés\n",
    "def determine_ressenti(mag_unif, prof):\n",
    "    # Si magnitude uniformisée et profondeur sont tous les deux NaN, retourner NaN\n",
    "    if pd.isna(mag_unif):\n",
    "        return pd.NA\n",
    "    \n",
    "    # Si magnitude uniformisée > 4, c'est ressenti peu importe la profondeur\n",
    "    if pd.notna(mag_unif) and mag_unif > 4.0:\n",
    "        return 'oui'\n",
    "    \n",
    "    # Si magnitude uniformisée >= 3 ET profondeur < 20, c'est ressenti\n",
    "    if pd.notna(mag_unif) and pd.notna(prof) and mag_unif >= 3.0 and prof < 20:\n",
    "        return 'oui'\n",
    "    \n",
    "    # Sinon non ressenti\n",
    "    return 'non'\n",
    "\n",
    "df['ressenti'] = df.apply(lambda row: determine_ressenti(row['mag_uniforme'], row['profondeur_km']), axis=1)\n",
    "\n",
    "# Placer la colonne \"ressenti\" en avant-dernière position\n",
    "cols = list(df.columns)\n",
    "cols.insert(-1, cols.pop(cols.index('ressenti')))\n",
    "df = df[cols]\n",
    "\n",
    "df.to_parquet('../data/STEP11_earthquakes.parquet')\n",
    "print(\"Colonne 'ressenti' créée et fichier '../data/STEP11_earthquakes.parquet' sauvegardé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc443b1",
   "metadata": {},
   "source": [
    "### Création d'un petit parquet pour l'interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "32435778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier '../data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP11_earthquakes.parquet')\n",
    "\n",
    "# Extraire les 10 premières lignes\n",
    "df_lite = df.head(10)\n",
    "\n",
    "# Sauvegarder en parquet lite\n",
    "df_lite.to_parquet('../data/earthquakes_lite.parquet')\n",
    "\n",
    "print(\"Fichier '../data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efee223",
   "metadata": {},
   "source": [
    "### Suppression des parquets intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1d101374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supprimé : ../data\\STEP01_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP02_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP03_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP04_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP05_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP06_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP07_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP08_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP09_earthquakes.parquet\n",
      "Supprimé : ../data\\STEP10_earthquakes.parquet\n",
      "Nettoyage terminé.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Lister tous les fichiers STEP*.parquet dans le dossier ../data/\n",
    "step_files = glob.glob('../data/STEP*.parquet')\n",
    "\n",
    "# Supprimer tous les fichiers STEP* SAUF STEP11\n",
    "for f in step_files:\n",
    "    # Garder seulement STEP11\n",
    "    if 'STEP11' not in f:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "            print(f\"Supprimé : {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la suppression de {f} : {e}\")\n",
    "print(\"Nettoyage terminé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c1b3",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2923bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes par colonne :\n",
      "ID                               0\n",
      "date                             0\n",
      "lieu                             0\n",
      "magnitude                    20342\n",
      "type_magnitude               20405\n",
      "latitude                         0\n",
      "longitude                        0\n",
      "profondeur_km                    0\n",
      "mag_uniforme                 61435\n",
      "nb_stations_localisation     97532\n",
      "nb_stations_magnitude       177082\n",
      "ecart_azimut                 90784\n",
      "rms                           2589\n",
      "erreur_horiz                141364\n",
      "erreur_profondeur             5037\n",
      "erreur_magnitude            221474\n",
      "ressenti                     61435\n",
      "date_maj_infos                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('../data/STEP11_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
