{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a655be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427aee",
   "metadata": {},
   "source": [
    "### Infos du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d04fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d3f67",
   "metadata": {},
   "source": [
    "### Types de variables avant conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1b35",
   "metadata": {},
   "source": [
    "### Conversion des types des variables et création du parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "# tout convertir\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# puis les dates\n",
    "date_columns = ['time', 'updated']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "df.to_parquet('data/STEP01_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf768b",
   "metadata": {},
   "source": [
    "### Vérification conversion des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")\n",
    "\n",
    "# Compter le nombre de lignes du fichier\n",
    "total_rows = len(df)\n",
    "print(f\"Nombre total de lignes dans le fichier: {total_rows}\")\n",
    "\n",
    "# Compter le nombre de variables de type date (non-null) dans 'time' et 'updated'\n",
    "time_date_count = df['time'].notnull().sum()\n",
    "updated_date_count = df['updated'].notnull().sum()\n",
    "\n",
    "print(f\"Nombre de variables de type date dans 'time': {time_date_count}\")\n",
    "print(f\"Nombre de variables de type date dans 'updated': {updated_date_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cc592",
   "metadata": {},
   "source": [
    "### Doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for strictly identical rows\n",
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'Number of strictly identical rows: {num_duplicates}')\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    duplicated_rows = df[df.duplicated(keep=False)]\n",
    "    display(duplicated_rows.head(10)) \n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate 'id' values\n",
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "total_ids = len(df)\n",
    "unique_ids = df['id'].nunique()\n",
    "duplicate_ids_count = total_ids - unique_ids\n",
    "\n",
    "print(f\"Total 'id' entries: {total_ids}\")\n",
    "print(f\"Unique 'id' entries: {unique_ids}\")\n",
    "print(f\"Number of identical 'id's (duplicates): {duplicate_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Supprimer les doublons\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "after = len(df)\n",
    "\n",
    "# Calculer le nombre de doublons supprimés\n",
    "removed = before - after\n",
    "\n",
    "df.to_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "print(f\"Fichier 'data/STEP02_earthquakes.parquet' créé sans doublons.\")\n",
    "print(f\"Nombre de doublons supprimés : {removed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dc092",
   "metadata": {},
   "source": [
    "### Les valeurs 0 de nst et magNst passent en valeurs vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "# Remplacer les 0 par NaN dans nst et magNst\n",
    "df['nst'] = df['nst'].replace(0, pd.NA)\n",
    "df['magNst'] = df['magNst'].replace(0, pd.NA)\n",
    "\n",
    "# Sauvegarder le parquet mis à jour\n",
    "df.to_parquet('data/STEP03_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815d8ab",
   "metadata": {},
   "source": [
    "### Vérification valeurs faussement différentes dans \"place\" (ex : central East Pacific Rise, Central East Pacific Rise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17026301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n",
    "\n",
    "# Compter les occurrences de chaque place\n",
    "place_counts = df['place'].value_counts()\n",
    "\n",
    "# Places qui apparaissent plus d'une fois\n",
    "duplicates = place_counts[place_counts > 1]\n",
    "\n",
    "print(f\"Nombre de places qui apparaissent plus d'une fois : {len(duplicates)}\")\n",
    "print(f\"Total d'occurrences pour ces places : {duplicates.sum()}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Exemples de places avec leurs occurrences :\")\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_place(place):\n",
    "    if pd.isna(place):\n",
    "        return place\n",
    "    # Remove accents\n",
    "    place = unicodedata.normalize('NFD', place).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    place = place.lower()\n",
    "    # Remove punctuation\n",
    "    place = re.sub(r'[^\\w\\s]', '', place)\n",
    "    # Remove extra spaces\n",
    "    place = ' '.join(place.split())\n",
    "    return place\n",
    "\n",
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Apply cleaning to the 'place' column\n",
    "df['place_cleaned'] = df['place'].apply(clean_place)\n",
    "\n",
    "# Count unique cleaned places\n",
    "unique_clean = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' après nettoyage : {unique_clean}\")\n",
    "\n",
    "# Group by cleaned place and show examples of groupings\n",
    "grouped = df.groupby('place_cleaned')['place'].unique()\n",
    "grouped_multiple = grouped[grouped.apply(len) > 1]\n",
    "\n",
    "print(\"Exemples de lieux regroupés après nettoyage :\")\n",
    "for cleaned, originals in grouped_multiple.head(10).items():\n",
    "    print(f\"Nettoyé: '{cleaned}'\")\n",
    "    print(f\"Originaux: {list(originals)}\")\n",
    "    print(\"---\")\n",
    "\n",
    "df = df.drop(columns=['place'])\n",
    "\n",
    "# Enregistrer le parquet avec la colonne 'place' nettoyée\n",
    "df.to_parquet('data/STEP04_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06820b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce9333",
   "metadata": {},
   "source": [
    "### Ajout colonne magnitude uniformisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_magnitude(row):\n",
    "    mag = row[\"mag\"]\n",
    "    mtype = row[\"magType\"]\n",
    "\n",
    "    if pd.isna(mag) or pd.isna(mtype):\n",
    "        return np.nan\n",
    "\n",
    "    mtype = str(mtype).lower()  # normalisation\n",
    "\n",
    "    if mtype in [\"ml\", \"mlg\", \"mlr\"]:  \n",
    "        return mag\n",
    "\n",
    "    if mtype in [\"md\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"mw\", \"mwc\", \"mwb\", \"mwr\", \"mww\"]:\n",
    "        return (mag - 0.9) / 0.67\n",
    "\n",
    "    if mtype in [\"mc\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"ma\"]:\n",
    "        return mag  \n",
    "\n",
    "    return pd.NA\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Appliquer la fonction de conversion\n",
    "df['mag_uniform'] = df.apply(convert_magnitude, axis=1)\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('data/STEP05_earthquakes.parquet')\n",
    "\n",
    "print(\"Nouveau fichier 'data/STEP05_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a596417",
   "metadata": {},
   "source": [
    "### Suppression colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP05_earthquakes.parquet')\n",
    "\n",
    "df = df.drop(columns=['net', 'locationSource', 'magSource', 'status', 'dmin'])\n",
    "\n",
    "df.to_parquet('data/STEP06_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa295",
   "metadata": {},
   "source": [
    "### Réorganisation du parquet final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger ton fichier\n",
    "df = pd.read_parquet(\"data/STEP06_earthquakes.parquet\")\n",
    "\n",
    "# Dictionnaire de renommage\n",
    "rename_map = {\n",
    "    \"id\": \"ID\",\n",
    "    \"time\": \"date\",\n",
    "    \"updated\": \"date_maj_infos\",\n",
    "\n",
    "    \"depth\": \"profondeur_km\",\n",
    "\n",
    "    \"mag\": \"magnitude\",\n",
    "    \"magType\": \"type_magnitude\",\n",
    "    \"magError\": \"erreur_magnitude\",\n",
    "    \"magNst\": \"nb_stations_magnitude\",\n",
    "\n",
    "    \"nst\": \"nb_stations_localisation\",\n",
    "    \"gap\": \"ecart_azimut\",\n",
    "    \"rms\": \"rms\",\n",
    "\n",
    "    \"horizontalError\": \"erreur_horiz\",\n",
    "    \"depthError\": \"erreur_profondeur\",\n",
    "\n",
    "    \"place_cleaned\": \"lieu\",\n",
    "    \"mag_uniform\": \"mag_uniforme\"\n",
    "}\n",
    "\n",
    "# Renommer les colonnes\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Nouvel ordre des colonnes\n",
    "new_order = [\n",
    "    \"ID\", \"date\",\"lieu\",\"magnitude\", \"type_magnitude\",\"latitude\", \"longitude\", \"profondeur_km\",  \"mag_uniforme\",\"nb_stations_localisation\", \"nb_stations_magnitude\",\"ecart_azimut\", \"rms\",\"erreur_horiz\",\"erreur_profondeur\",\"erreur_magnitude\",\"reseau\",\"type\", \"date_maj_infos\"\n",
    "]\n",
    "\n",
    "# Garder uniquement les colonnes existantes\n",
    "new_order = [col for col in new_order if col in df.columns]\n",
    "\n",
    "# Réorganiser\n",
    "df = df[new_order]\n",
    "\n",
    "df.to_parquet('data/STEP07_earthquakes.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb4c48",
   "metadata": {},
   "source": [
    "### Suppression des évènements autres que \"earthquake\" et suppression de la colonne \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c55f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP07_earthquakes.parquet')\n",
    "\n",
    "# Filtrer pour garder uniquement les événements de type 'earthquake'\n",
    "df = df[df['type'] == 'earthquake']\n",
    "\n",
    "# Supprimer la colonne 'type'\n",
    "df = df.drop(columns=['type'])\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('data/STEP08_earthquakes.parquet')\n",
    "\n",
    "print(\"Fichier 'data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc443b1",
   "metadata": {},
   "source": [
    "### Création d'un petit parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32435778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP08_earthquakes.parquet')\n",
    "\n",
    "# Extraire les 10 premières lignes\n",
    "df_lite = df.head(10)\n",
    "\n",
    "# Sauvegarder en parquet lite\n",
    "df_lite.to_parquet('data/earthquakes_lite.parquet')\n",
    "\n",
    "print(\"Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c1b3",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes par colonne :\n",
      "ID                                0\n",
      "date                              0\n",
      "lieu                             11\n",
      "magnitude                    155698\n",
      "type_magnitude               166609\n",
      "latitude                          0\n",
      "longitude                         0\n",
      "profondeur_km                     9\n",
      "mag_uniforme                 701438\n",
      "nb_stations_localisation    1204826\n",
      "nb_stations_magnitude       1091476\n",
      "ecart_azimut                 834294\n",
      "rms                          210659\n",
      "erreur_horiz                1524519\n",
      "erreur_profondeur            603806\n",
      "erreur_magnitude            1772000\n",
      "type                              0\n",
      "date_maj_infos                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP08_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
