{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a655be78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427aee",
   "metadata": {},
   "source": [
    "### Infos du csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d04fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3272774, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>net</th>\n",
       "      <th>id</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>37.003502</td>\n",
       "      <td>-117.996834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci37038459</td>\n",
       "      <td>2016-04-02T20:22:05.312Z</td>\n",
       "      <td>29km NE of Independence, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>35.642788</td>\n",
       "      <td>-120.933601</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>mh</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci11092098</td>\n",
       "      <td>2016-01-29T01:43:14.870Z</td>\n",
       "      <td>11km SSW of Lake Nacimiento, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>34.164520</td>\n",
       "      <td>-118.185036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci15086796</td>\n",
       "      <td>2016-04-02T17:20:31.235Z</td>\n",
       "      <td>4km S of La Canada Flintridge, CA</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>33.836494</td>\n",
       "      <td>-116.781868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci14891508</td>\n",
       "      <td>2016-04-02T14:10:48.389Z</td>\n",
       "      <td>9km S of Cabazon, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01T00:00:00.0Z</td>\n",
       "      <td>33.208477</td>\n",
       "      <td>-115.476997</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci10925125</td>\n",
       "      <td>2016-04-02T04:32:22.103Z</td>\n",
       "      <td>5km SE of Niland, CA</td>\n",
       "      <td>sonic boom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>ci</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time   latitude   longitude  depth   mag magType  nst  \\\n",
       "0  1970-01-01T00:00:00.0Z  37.003502 -117.996834    0.0  0.00      mh  0.0   \n",
       "1  1970-01-01T00:00:00.0Z  35.642788 -120.933601    5.0  1.99      mh  2.0   \n",
       "2  1970-01-01T00:00:00.0Z  34.164520 -118.185036    0.0  0.00      mh  NaN   \n",
       "3  1970-01-01T00:00:00.0Z  33.836494 -116.781868    0.0  0.00      mh  NaN   \n",
       "4  1970-01-01T00:00:00.0Z  33.208477 -115.476997    5.0  0.00      mh  NaN   \n",
       "\n",
       "   gap  dmin  rms net          id                   updated  \\\n",
       "0  NaN   NaN  NaN  ci  ci37038459  2016-04-02T20:22:05.312Z   \n",
       "1  NaN   NaN  NaN  ci  ci11092098  2016-01-29T01:43:14.870Z   \n",
       "2  NaN   NaN  NaN  ci  ci15086796  2016-04-02T17:20:31.235Z   \n",
       "3  NaN   NaN  NaN  ci  ci14891508  2016-04-02T14:10:48.389Z   \n",
       "4  NaN   NaN  NaN  ci  ci10925125  2016-04-02T04:32:22.103Z   \n",
       "\n",
       "                               place        type  horizontalError  depthError  \\\n",
       "0        29km NE of Independence, CA  sonic boom              NaN         NaN   \n",
       "1    11km SSW of Lake Nacimiento, CA  earthquake              NaN         NaN   \n",
       "2  4km S of La Canada Flintridge, CA  earthquake              NaN         NaN   \n",
       "3               9km S of Cabazon, CA  sonic boom              NaN         NaN   \n",
       "4               5km SE of Niland, CA  sonic boom              NaN         NaN   \n",
       "\n",
       "   magError  magNst    status locationSource magSource  \n",
       "0       NaN     0.0  reviewed             ci        ci  \n",
       "1       NaN     0.0  reviewed             ci        ci  \n",
       "2       NaN     0.0  reviewed             ci        ci  \n",
       "3       NaN     0.0  reviewed             ci        ci  \n",
       "4       NaN     0.0  reviewed             ci        ci  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "print('Shape:', df.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d3f67",
   "metadata": {},
   "source": [
    "### Types de variables avant conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f5b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f1b35",
   "metadata": {},
   "source": [
    "### Conversion des types des variables et création du parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/earthquakes.csv')\n",
    "\n",
    "# tout convertir\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# puis les dates\n",
    "date_columns = ['time', 'updated']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], utc=True, errors='coerce')\n",
    "\n",
    "df.to_parquet('data/STEP01_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf768b",
   "metadata": {},
   "source": [
    "### Vérification conversion des dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94131a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs vides dans 'time' et 'updated'\n",
    "time_nulls = df['time'].isnull().sum()\n",
    "updated_nulls = df['updated'].isnull().sum()\n",
    "\n",
    "print(f\"Nombre de valeurs vides dans 'time': {time_nulls}\")\n",
    "print(f\"Nombre de valeurs vides dans 'updated': {updated_nulls}\")\n",
    "\n",
    "# Compter le nombre de lignes du fichier\n",
    "total_rows = len(df)\n",
    "print(f\"Nombre total de lignes dans le fichier: {total_rows}\")\n",
    "\n",
    "# Compter le nombre de variables de type date (non-null) dans 'time' et 'updated'\n",
    "time_date_count = df['time'].notnull().sum()\n",
    "updated_date_count = df['updated'].notnull().sum()\n",
    "\n",
    "print(f\"Nombre de variables de type date dans 'time': {time_date_count}\")\n",
    "print(f\"Nombre de variables de type date dans 'updated': {updated_date_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cc592",
   "metadata": {},
   "source": [
    "### Doublons ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for strictly identical rows\n",
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "print(f'Number of strictly identical rows: {num_duplicates}')\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    duplicated_rows = df[df.duplicated(keep=False)]\n",
    "    display(duplicated_rows.head(10)) \n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate 'id' values\n",
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "total_ids = len(df)\n",
    "unique_ids = df['id'].nunique()\n",
    "duplicate_ids_count = total_ids - unique_ids\n",
    "\n",
    "print(f\"Total 'id' entries: {total_ids}\")\n",
    "print(f\"Unique 'id' entries: {unique_ids}\")\n",
    "print(f\"Number of identical 'id's (duplicates): {duplicate_ids_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP01_earthquakes.parquet')\n",
    "\n",
    "before = len(df)\n",
    "\n",
    "# Supprimer les doublons\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "after = len(df)\n",
    "\n",
    "# Calculer le nombre de doublons supprimés\n",
    "removed = before - after\n",
    "\n",
    "df.to_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "print(f\"Fichier 'data/STEP02_earthquakes.parquet' créé sans doublons.\")\n",
    "print(f\"Nombre de doublons supprimés : {removed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24dc092",
   "metadata": {},
   "source": [
    "### Les valeurs 0 de nst et magNst passent en valeurs vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP02_earthquakes.parquet')\n",
    "\n",
    "# Remplacer les 0 par NaN dans nst et magNst\n",
    "df['nst'] = df['nst'].replace(0, pd.NA)\n",
    "df['magNst'] = df['magNst'].replace(0, pd.NA)\n",
    "\n",
    "# Sauvegarder le parquet mis à jour\n",
    "df.to_parquet('data/STEP03_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815d8ab",
   "metadata": {},
   "source": [
    "### Vérification valeurs faussement différentes dans \"place\" (ex : central East Pacific Rise, Central East Pacific Rise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17026301",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n",
    "\n",
    "# Compter les occurrences de chaque place\n",
    "place_counts = df['place'].value_counts()\n",
    "\n",
    "# Places qui apparaissent plus d'une fois\n",
    "duplicates = place_counts[place_counts > 1]\n",
    "\n",
    "print(f\"Nombre de places qui apparaissent plus d'une fois : {len(duplicates)}\")\n",
    "print(f\"Total d'occurrences pour ces places : {duplicates.sum()}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"Exemples de places avec leurs occurrences :\")\n",
    "    print(duplicates.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_place(place):\n",
    "    if pd.isna(place):\n",
    "        return place\n",
    "    # Remove accents\n",
    "    place = unicodedata.normalize('NFD', place).encode('ascii', 'ignore').decode('ascii')\n",
    "    # Convert to lowercase\n",
    "    place = place.lower()\n",
    "    # Remove punctuation\n",
    "    place = re.sub(r'[^\\w\\s]', '', place)\n",
    "    # Remove extra spaces\n",
    "    place = ' '.join(place.split())\n",
    "    return place\n",
    "\n",
    "df = pd.read_parquet('data/STEP03_earthquakes.parquet')\n",
    "\n",
    "# Apply cleaning to the 'place' column\n",
    "df['place_cleaned'] = df['place'].apply(clean_place)\n",
    "\n",
    "# Count unique cleaned places\n",
    "unique_clean = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' après nettoyage : {unique_clean}\")\n",
    "\n",
    "# Group by cleaned place and show examples of groupings\n",
    "grouped = df.groupby('place_cleaned')['place'].unique()\n",
    "grouped_multiple = grouped[grouped.apply(len) > 1]\n",
    "\n",
    "print(\"Exemples de lieux regroupés après nettoyage :\")\n",
    "for cleaned, originals in grouped_multiple.head(10).items():\n",
    "    print(f\"Nettoyé: '{cleaned}'\")\n",
    "    print(f\"Originaux: {list(originals)}\")\n",
    "    print(\"---\")\n",
    "\n",
    "df = df.drop(columns=['place'])\n",
    "\n",
    "# Enregistrer le parquet avec la colonne 'place' nettoyée\n",
    "df.to_parquet('data/STEP04_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06820b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Nombre de valeurs uniques dans 'place' original\n",
    "unique_original = df['place_cleaned'].nunique()\n",
    "print(f\"Nombre de valeurs uniques dans 'place' original : {unique_original}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce9333",
   "metadata": {},
   "source": [
    "### Ajout colonne magnitude uniformisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_magnitude(row):\n",
    "    mag = row[\"mag\"]\n",
    "    mtype = row[\"magType\"]\n",
    "\n",
    "    if pd.isna(mag) or pd.isna(mtype):\n",
    "        return np.nan\n",
    "\n",
    "    mtype = str(mtype).lower()  # normalisation\n",
    "\n",
    "    if mtype in [\"ml\", \"mlg\", \"mlr\"]:  \n",
    "        return mag\n",
    "\n",
    "    if mtype in [\"md\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"mw\", \"mwc\", \"mwb\", \"mwr\", \"mww\"]:\n",
    "        return (mag - 0.9) / 0.67\n",
    "\n",
    "    if mtype in [\"mc\"]:\n",
    "        return 0.85 * mag + 0.3\n",
    "\n",
    "    if mtype in [\"ma\"]:\n",
    "        return mag  \n",
    "\n",
    "    return pd.NA\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_parquet('data/STEP04_earthquakes.parquet')\n",
    "\n",
    "# Appliquer la fonction de conversion\n",
    "df['mag_uniform'] = df.apply(convert_magnitude, axis=1)\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('data/STEP05_earthquakes.parquet')\n",
    "\n",
    "print(\"Nouveau fichier 'data/STEP05_earthquakes.parquet' créé avec la colonne 'mag_uniform'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a596417",
   "metadata": {},
   "source": [
    "### Suppression colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/STEP05_earthquakes.parquet')\n",
    "\n",
    "df = df.drop(columns=['net', 'locationSource', 'magSource', 'status', 'dmin'])\n",
    "\n",
    "df.to_parquet('data/STEP06_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfa295",
   "metadata": {},
   "source": [
    "### Réorganisation du parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger ton fichier\n",
    "df = pd.read_parquet(\"data/STEP06_earthquakes.parquet\")\n",
    "\n",
    "# Dictionnaire de renommage\n",
    "rename_map = {\n",
    "    \"id\": \"ID\",\n",
    "    \"time\": \"date\",\n",
    "    \"updated\": \"date_maj_infos\",\n",
    "\n",
    "    \"depth\": \"profondeur_km\",\n",
    "\n",
    "    \"mag\": \"magnitude\",\n",
    "    \"magType\": \"type_magnitude\",\n",
    "    \"magError\": \"erreur_magnitude\",\n",
    "    \"magNst\": \"nb_stations_magnitude\",\n",
    "\n",
    "    \"nst\": \"nb_stations_localisation\",\n",
    "    \"gap\": \"ecart_azimut\",\n",
    "    \"rms\": \"rms\",\n",
    "\n",
    "    \"horizontalError\": \"erreur_horiz\",\n",
    "    \"depthError\": \"erreur_profondeur\",\n",
    "\n",
    "    \"place_cleaned\": \"lieu\",\n",
    "    \"mag_uniform\": \"mag_uniforme\"\n",
    "}\n",
    "\n",
    "# Renommer les colonnes\n",
    "df = df.rename(columns=rename_map)\n",
    "\n",
    "# Nouvel ordre des colonnes\n",
    "new_order = [\n",
    "    \"ID\", \"date\",\"lieu\",\"magnitude\", \"type_magnitude\",\"latitude\", \"longitude\", \"profondeur_km\",  \"mag_uniforme\",\"nb_stations_localisation\", \"nb_stations_magnitude\",\"ecart_azimut\", \"rms\",\"erreur_horiz\",\"erreur_profondeur\",\"erreur_magnitude\",\"reseau\",\"type\", \"date_maj_infos\"\n",
    "]\n",
    "\n",
    "# Garder uniquement les colonnes existantes\n",
    "new_order = [col for col in new_order if col in df.columns]\n",
    "\n",
    "# Réorganiser\n",
    "df = df[new_order]\n",
    "\n",
    "df.to_parquet('data/STEP07_earthquakes.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb4c48",
   "metadata": {},
   "source": [
    "### Suppression des évènements autres que \"earthquake\" et suppression de la colonne \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c55f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP07_earthquakes.parquet')\n",
    "\n",
    "# Filtrer pour garder uniquement les événements de type 'earthquake'\n",
    "df = df[df['type'] == 'earthquake']\n",
    "\n",
    "# Supprimer la colonne 'type'\n",
    "df = df.drop(columns=['type'])\n",
    "\n",
    "# Sauvegarder dans un nouveau parquet\n",
    "df.to_parquet('data/STEP08_earthquakes.parquet')\n",
    "\n",
    "print(\"Fichier 'data/STEP08_earthquakes.parquet' créé avec uniquement les événements 'earthquake' et colonne 'type' supprimée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af5112",
   "metadata": {},
   "source": [
    "### Garder de 2000 à 2005 uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577f0453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes entre 2000 et 2005 : 541525\n"
     ]
    }
   ],
   "source": [
    "# Filtrer les lignes dont la date est comprise entre 2000 et 2005 inclus\n",
    "df = pd.read_parquet('data/STEP08_earthquakes.parquet')\n",
    "\n",
    "mask_2000_2005 = (df['date'] >= '2000-01-01') & (df['date'] < '2006-01-01')\n",
    "df_2000_2005 = df[mask_2000_2005]\n",
    "\n",
    "print(f\"Nombre de lignes entre 2000 et 2005 : {len(df_2000_2005)}\")\n",
    "df_2000_2005.head()\n",
    "\n",
    "df_2000_2005.to_parquet('data/STEP09_earthquakes.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43fa7d",
   "metadata": {},
   "source": [
    "### Garder uniquement les USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 380123 lignes exportées\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "df = pd.read_parquet(\"data/STEP09_earthquakes.parquet\")\n",
    "\n",
    "# Convertir en GeoDataFrame (WGS84)\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry=gpd.points_from_xy(df[\"longitude\"], df[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Charger les pays\n",
    "world = gpd.read_file(\n",
    "    \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    ")\n",
    "\n",
    "usa = world[world[\"ADMIN\"] == \"United States of America\"]\n",
    "\n",
    "# Projection métrique US\n",
    "gdf_m = gdf.to_crs(\"EPSG:5070\")   # mètres\n",
    "usa_m = usa.to_crs(\"EPSG:5070\")\n",
    "\n",
    "# Buffer de 50 km autour des USA\n",
    "usa_buffer = usa_m.geometry.buffer(50 * 1000)\n",
    "\n",
    "# Filtrage spatial\n",
    "mask = gdf_m.geometry.within(usa_buffer.iloc[0])\n",
    "filtered = gdf.loc[mask].copy()\n",
    "\n",
    "# Export parquet\n",
    "filtered.drop(columns=\"geometry\").to_parquet(\"data/STEP10_earthquakes.parquet\")\n",
    "\n",
    "print(f\"✅ {len(filtered)} lignes exportées\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04651",
   "metadata": {},
   "source": [
    "### Ressenti ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca13594",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'ressenti' créée et fichier 'data/STEP11_earthquakes.parquet' sauvegardé.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP10_earthquakes.parquet')\n",
    "\n",
    "# Création de la colonne \"ressenti\" selon les critères donnés\n",
    "def determine_ressenti(mag_unif, prof):\n",
    "    # Si magnitude uniformisée et profondeur sont tous les deux NaN, retourner NaN\n",
    "    if pd.isna(mag_unif) and pd.isna(prof):\n",
    "        return pd.NA\n",
    "    \n",
    "    # Si magnitude uniformisée > 4, c'est ressenti peu importe la profondeur\n",
    "    if pd.notna(mag_unif) and mag_unif > 4.0:\n",
    "        return 'oui'\n",
    "    \n",
    "    # Si magnitude uniformisée > 3 ET profondeur < 20, c'est ressenti\n",
    "    if pd.notna(mag_unif) and pd.notna(prof) and mag_unif > 3.0 and prof < 20:\n",
    "        return 'oui'\n",
    "    \n",
    "    # Sinon non ressenti\n",
    "    return 'non'\n",
    "\n",
    "df['ressenti'] = df.apply(lambda row: determine_ressenti(row['mag_uniforme'], row['profondeur_km']), axis=1)\n",
    "\n",
    "# Placer la colonne \"ressenti\" en avant-dernière position\n",
    "cols = list(df.columns)\n",
    "cols.insert(-1, cols.pop(cols.index('ressenti')))\n",
    "df = df[cols]\n",
    "\n",
    "df.to_parquet('data/STEP11_earthquakes.parquet')\n",
    "print(\"Colonne 'ressenti' créée et fichier 'data/STEP11_earthquakes.parquet' sauvegardé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc443b1",
   "metadata": {},
   "source": [
    "### Création d'un petit parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32435778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP11_earthquakes.parquet')\n",
    "\n",
    "# Extraire les 10 premières lignes\n",
    "df_lite = df.head(10)\n",
    "\n",
    "# Sauvegarder en parquet lite\n",
    "df_lite.to_parquet('data/earthquakes_lite.parquet')\n",
    "\n",
    "print(\"Fichier 'data/earthquakes_lite.parquet' créé avec les 10 premières lignes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83c1b3",
   "metadata": {},
   "source": [
    "### Valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2923bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes par colonne :\n",
      "ID                               0\n",
      "date                             0\n",
      "lieu                             0\n",
      "magnitude                    20342\n",
      "type_magnitude               20405\n",
      "latitude                         0\n",
      "longitude                        0\n",
      "profondeur_km                    0\n",
      "mag_uniforme                 61435\n",
      "nb_stations_localisation     97532\n",
      "nb_stations_magnitude       177082\n",
      "ecart_azimut                 90784\n",
      "rms                           2589\n",
      "erreur_horiz                141364\n",
      "erreur_profondeur             5037\n",
      "erreur_magnitude            221474\n",
      "ressenti                    375876\n",
      "date_maj_infos                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet('data/STEP11_earthquakes.parquet')\n",
    "\n",
    "# Compter les valeurs manquantes par colonne\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Nombre de valeurs manquantes par colonne :\")\n",
    "print(missing_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
